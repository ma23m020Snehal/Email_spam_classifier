# -*- coding: utf-8 -*-
"""finalcode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Rwp8FYibX5fj7IcjLk7smNWfFYXnWNS
"""

####################################################################################
# Step 1: importing the important libraries Importing necessary libraries
####################################################################################
import os
import pandas as pd
import numpy as np
import string
import math
import random
from sklearn.svm import SVC  # Import SVC from scikit-learn


#####################################################################################
# Step 2: Loading the data and splitting it into train and test
####################################################################################
def loading_dataset(test_size=0.2):
    data = pd.read_csv('train1.csv', encoding='latin-1')
    data = data[['target', 'text']]
    data.columns = ['label', 'text']
    #labels to 0 (ham) and 1 (spam)
    data['label'] = data['label'].map({'ham': 0, 'spam': 1})
    #Shuffle the data
    data = data.sample(frac=1, random_state=42).reset_index(drop=True)
    #split into training and test sets
    split_index = int(len(data) * (1 - test_size))
    train_data = data[:split_index]
    test_data = data[split_index:]
    return train_data, test_data


########################################################################################################
# Step 3: Preprocessing the texts
# Cleans text data for analysis by defining a set of common stopwords to remove,removes punctuation ,Converts text to lowercase to ensure uniformity,
# Splits the text into words and removes stopwords and Returns a list of processed words.
#############################################################################################################
def text_preprocess(text):
    stopwords = set(['the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'as', 'this', 'that', 'it', 'at','I','ME','too',
                     'withm','annd','FORR','these','a', 'an', 'the'])
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.lower()
    words = [word for word in text.split() if word not in stopwords]
    return words


#################################################################################################################
# Step 4: Creating vocab of the most frequent words in the dataset
################################################################################################################
def build_vocab(data, max_features=10000):
    word_counts = {}
    for text in data['text']:
        words = text_preprocess(text)
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
    sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
    vocab = [word for word, count in sorted_words[:max_features]]
    print(len(vocab))
    return vocab

#################################################################################################################
# Step 5: Transforming text data into numerical feature matrices for model training
################################################################################################################
def convert_text_into_features(data, vocab):
    data = data.reset_index(drop=True)  # Reset index to ensure it starts from 0
    num_emails = len(data)   # Determines the number of emails
    num_words = len(vocab) # Determines the vocab size.
    word_index = {word: i for i, word in enumerate(vocab)}   # create a mapping of words to their indices
    X = np.zeros((num_emails, num_words))
    y = data['label'].values  # Labels are 0 -ham or 1-spam

    for idx, row in data.iterrows():
        words = text_preprocess(row['text'])
        for word in words:
            if word in word_index:
                X[idx, word_index[word]] += 1
    return X, y

###############################################################################################################
# Step 6: Defining the models
###############################################################################################################
# Step 6A : K-Nearest Neighbors
##########################################################################################################
def knn_train(X_train, y_train):
    return X_train, y_train

# Implements the K-Nearest Neighbors (KNN) algorithm to make predictions on test samples based on the closest samples from the training set
def knn_prerd(X_train, y_train, X_test, k=5):
    predictions = []
    for x_test in X_test:
        distances = np.sqrt(np.sum((X_train - x_test) ** 2, axis=1))  # Calculating the Euclidean distances to each sample in X_train.
        k_indices = distances.argsort()[:k]
        k_labels = y_train[k_indices]
        prediction = int(np.round(np.mean(k_labels)))   # Using a majority vote (or mean) of their labels to decide the label of the test sample.
        predictions.append(prediction)
    return np.array(predictions)
################################################################################################################
# Step 6B: Logistic regression model
################################################################################################################

# defining the sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Logistic Regression

#This function trains a logistic regression model on a given dataset using gradient descent with L2 regularization
# and class wts to handle imbalanced classes.
def logistic_reg_train(X, y, learning_rate=0.01, epochs=2500, lambda_reg=0.10):
    num_samples, num_features = X.shape
    wts = np.zeros(num_features)
    bias = 0

    # Calculate class wts
    num_positive = np.sum(y)
    num_negative = len(y) - num_positive
    weight_positive = num_negative / len(y)
    weight_negative = num_positive / len(y)

    for epoch in range(epochs):
        linear_model = np.dot(X, wts) + bias
        predictions = sigmoid(linear_model)

        # Compute gradients with class wts
        errors = predictions - y
        errors_weighted = errors.copy()
        errors_weighted[y == 1] *= weight_positive
        errors_weighted[y == 0] *= weight_negative

        dw = (1 / num_samples) * np.dot(X.T, errors_weighted) + (lambda_reg / num_samples) * wts
        db = (1 / num_samples) * np.sum(errors_weighted)

        wts -= learning_rate * dw
        bias -= learning_rate * db

    return wts, bias

def logistic_reg_pred(X, wts, bias, threshold=0.50):
    linear_model = np.dot(X, wts) + bias
    predictions = sigmoid(linear_model)
    return (predictions >= threshold).astype(int)


##################################################################################################
# Step 6C: SVM  : directly implemented from scikit learn library
##################################################################################################

#################################################################################################################
# Step 6D :Naive Bayes CLassifier
################################################################################################################

def find_word_freq(data, vocab):
    # computes how often each vocab word appears in spam and ham emails
    spam_word_counts = dict.fromkeys(vocab, 0)
    ham_word_counts = dict.fromkeys(vocab, 0)

    total_spam = 0
    total_ham = 0

    for index, row in data.iterrows():
        words = text_preprocess(row['text'])
        if row['label'] == 1:  # Spam
            total_spam += 1
            for word in words:
                if word in spam_word_counts:
                    spam_word_counts[word] += 1
        else:  # Ham
            total_ham += 1
            for word in words:
                if word in ham_word_counts:
                    ham_word_counts[word] += 1

    return spam_word_counts, ham_word_counts, total_spam, total_ham


# uses these frequencies to calculate prior probabilities and likelihood probabilities for each word
def naive_bayes_train(spam_word_counts, ham_word_counts, total_spam, total_ham, vocab):
    total_messages = total_spam + total_ham
    prior_spam = total_spam / total_messages
    prior_ham = total_ham / total_messages

    total_words_spam = sum(spam_word_counts.values())
    total_words_ham = sum(ham_word_counts.values())

    spam_word_probs = {}
    ham_word_probs = {}
    vocab_size = len(vocab)

    for word in vocab:
        spam_word_probs[word] = (spam_word_counts[word] + 1) / (total_words_spam + vocab_size)
        ham_word_probs[word] = (ham_word_counts[word] + 1) / (total_words_ham + vocab_size)

    return prior_spam, prior_ham, spam_word_probs, ham_word_probs

def naive_bayes_pred(X, prior_spam, prior_ham, spam_word_probs, ham_word_probs, vocab):
  #calculates the posterior probability of each class for new emails, using logarithms to prevent underflow issues.
    predictions = []
    for x in X:
            # the log-probabilities for both spam and ham are initialized using the priors:
        log_prob_spam = math.log(prior_spam)
        log_prob_ham = math.log(prior_ham)
         # for each word in the vocab (indexed by idx),
        # if the word appears in the email (count > 0), it contributes to the log-probabilities:
        for idx, count in enumerate(x):
            if count > 0:
                word = vocab[idx]
                log_prob_spam += count * math.log(spam_word_probs[word])
                log_prob_ham += count * math.log(ham_word_probs[word])

       # The class with the higher log-probability is chosen as the predicted label.
        prediction = 1 if log_prob_spam > log_prob_ham else 0
        predictions.append(prediction)
    return np.array(predictions)
# This step uses the logarithmic form to avoid underflow issues. Multiplying many probabilities can lead to very small numbers;
#using logarithms helps keep values manageable and computationally stable.




##################################################################################################
# Step 6E: Perceptron
##################################################################################################

def perceptron_train(X, y, learning_rate=0.01, epochs=1500):
    num_samples, num_features = X.shape
    wts = np.zeros(num_features)
    bias = 0

    y_perceptron = np.where(y == 0, -1, 1)

    for epoch in range(epochs):
        for idx, x_i in enumerate(X):
            linear_output = np.dot(x_i, wts) + bias
            prediction = 1 if linear_output >= 0 else -1
            if y_perceptron[idx] != prediction:
                wts += learning_rate * y_perceptron[idx] * x_i
                bias += learning_rate * y_perceptron[idx]
    return wts, bias

def perceptron_pred(X, wts, bias):
    linear_output = np.dot(X, wts) + bias
    return np.where(linear_output >= 0, 1, 0)


##############################################################################################################
# Step 7: Evaluate the models
##############################################################################################################
def model_evaluation(y_true, y_pred):
    accuracy = np.mean(y_true == y_pred)
    return accuracy


##############################################################################################################
# Step 8: The main function
#############################################################################################################

def main():
    # Load and split the dataset
    train_data, test_data = loading_dataset()
    print(f"Training data: {len(train_data)} emails")
    print(f"Test data: {len(test_data)} emails")

    # Build vocabulary and convert text to features
    vocab = build_vocab(train_data)
    print(f"vocab size: {len(vocab)} words.")
    X_train, y_train = convert_text_into_features(train_data, vocab)
    X_test, y_test = convert_text_into_features(test_data, vocab)
    print("Feature matrices created.")

    # Model Accuracy Results
    model_accuracies = []

    # KNN
    X_knn_train, y_knn_train = knn_train(X_train, y_train)
    y_pred_knn = knn_prerd(X_knn_train, y_knn_train, X_test, k=5)
    accuracy_knn = model_evaluation(y_test, y_pred_knn)
    model_accuracies.append({"Model": "K-Nearest Neighbors", "Accuracy": accuracy_knn})

    # Logistic Regression
    weights_lr, bias_lr = logistic_reg_train(X_train, y_train)
    y_pred_lr = logistic_reg_pred(X_test, weights_lr, bias_lr)
    accuracy_lr = model_evaluation(y_test, y_pred_lr)
    model_accuracies.append({"Model": "Logistic Regression", "Accuracy": accuracy_lr})

    # SVM using scikit-learn
    svm_model = SVC(kernel='linear', C=1.0)
    svm_model.fit(X_train, y_train)
    y_pred_svm = svm_model.predict(X_test)
    accuracy_svm = model_evaluation(y_test, y_pred_svm)
    model_accuracies.append({"Model": "Support Vector Machine", "Accuracy": accuracy_svm})

    # Naive Bayes
    spam_word_counts, ham_word_counts, total_spam, total_ham = find_word_freq(train_data, vocab)
    prior_spam, prior_ham, spam_word_probs, ham_word_probs = naive_bayes_train(
        spam_word_counts, ham_word_counts, total_spam, total_ham, vocab)
    y_pred_nb = naive_bayes_pred(X_test, prior_spam, prior_ham, spam_word_probs, ham_word_probs, vocab)
    accuracy_nb = model_evaluation(y_test, y_pred_nb)
    model_accuracies.append({"Model": "Naive Bayes", "Accuracy": accuracy_nb})

    # Perceptron
    weights_perceptron, bias_perceptron = perceptron_train(X_train, y_train)
    y_pred_perceptron = perceptron_pred(X_test, weights_perceptron, bias_perceptron)
    accuracy_perceptron = model_evaluation(y_test, y_pred_perceptron)
    model_accuracies.append({"Model": "Perceptron", "Accuracy": accuracy_perceptron})

    # Voting Ensemble
    y_preds = np.vstack([y_pred_nb, y_pred_lr, y_pred_knn, y_pred_svm, y_pred_perceptron])
    y_pred_ensemble = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=y_preds)
    accuracy_ensemble = model_evaluation(y_test, y_pred_ensemble)
    model_accuracies.append({"Model": "Voting Ensemble", "Accuracy": accuracy_ensemble})

    # Display Model Accuracies in Table Format
    accuracy_df = pd.DataFrame(model_accuracies)
    print("\nModel Accuracies:")
    print(accuracy_df.to_string(index=False))

    # Prediction on new emails
    test_folder = 'test'
    predictions = []
    word_index = {word: i for i, word in enumerate(vocab)}

    for filename in os.listdir(test_folder):
        if filename.startswith('email') and filename.endswith('.txt'):
            filepath = os.path.join(test_folder, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                email_text = f.read()
                words = text_preprocess(email_text)
                x_new = np.zeros(len(vocab))
                for word in words:
                    if word in word_index:
                        x_new[word_index[word]] += 1

                # Collect predictions from each model for voting ensemble
                preds = []
                # KNN
                pred_knn = knn_prerd(X_knn_train, y_knn_train, x_new.reshape(1, -1), k=5)[0]
                preds.append(pred_knn)

                # Logistic Regression
                pred_lr = logistic_reg_pred(x_new.reshape(1, -1), weights_lr, bias_lr)[0]
                preds.append(pred_lr)

                # SVM
                pred_svm = svm_model.predict(x_new.reshape(1, -1))[0]
                preds.append(pred_svm)

                # Naive Bayes
                x_new_nb = x_new.reshape(1, -1)
                pred_nb = naive_bayes_pred(x_new_nb, prior_spam, prior_ham, spam_word_probs, ham_word_probs, vocab)[0]
                preds.append(pred_nb)

                # Perceptron
                pred_perceptron = perceptron_pred(x_new.reshape(1, -1), weights_perceptron, bias_perceptron)[0]
                preds.append(pred_perceptron)

                # Voting Ensemble Prediction
                pred_ensemble = np.bincount(preds).argmax()
                result = 'Spam (+1)' if pred_ensemble == 1 else 'Ham (0)'
                predictions.append({"Filename": filename, "Voting Ensemble Prediction": result})

    # Display Predictions in Table Format
    predictions_df = pd.DataFrame(predictions)
    print("\nPredictions on New Emails:")
    print(predictions_df.to_string(index=False))

if __name__ == "__main__":
    main()

